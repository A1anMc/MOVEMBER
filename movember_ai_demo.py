#!/usr/bin/env python3
"""
Movember AI Rules System Demo

Comprehensive demonstration of the Movember AI Rules System v1.1
Showcasing AI behaviour, impact reporting, grant evaluation, and weekly refactoring.
"""

import asyncio
import json
import time
from datetime import datetime

from rules.domains.movember_ai import (
    MovemberAIRulesEngine,
    create_movember_engine,
    validate_movember_operation,
    run_movember_impact_analysis,
    evaluate_grant_application,
    run_weekly_refactor
)
from rules.types import ExecutionContext, ContextType


def print_separator(title: str):
    """Print a formatted separator."""
    print("\n" + "=" * 70)
    print(f" {title}")
    print("=" * 70)


def print_rule_results(results):
    """Print rule execution results in a formatted way."""
    for result in results:
        status = "‚úÖ PASSED" if result.success else "‚ùå FAILED"
        print(f"\n{status} - {result.rule_name}")
        print(f"  Execution Time: {result.execution_time:.3f}s")

        if result.conditions_met is not None:
            print(f"  Conditions Met: {'Yes' if result.conditions_met else 'No'}")

        if result.action_results:
            print("  Actions Executed:")
            for action_result in result.action_results:
                action_status = "‚úÖ" if action_result.success else "‚ùå"
                print(f"    {action_status} {action_result.action_name}: {action_result.result}")

        if result.error:
            print(f"  Error: {result.error}")


async def demo_ai_behaviour_rules():
    """Demonstrate AI behaviour rules."""
    print_separator("AI BEHAVIOUR RULES DEMO")

    # Create Movember AI engine
    engine = create_movember_engine()

    print("üß† Testing AI Behaviour Rules")
    print("Rules ensure professional tone, data integrity, and mission alignment")

    # Test case 1: Professional tone for executive audience
    print("\nüß™ Test Case 1: Executive Communication")
    executive_context = ExecutionContext(
        context_type=ContextType.BUSINESS_PROCESS,
        context_id=f"exec_{int(time.time())}",
        data={
            'project_id': 'movember',
            'agent': {
                'role': 'impact_intelligence',
                'audience': 'executive',
                'confidence': 0.9
            },
            'output': {
                'audience': 'executive',
                'formality_level': 'high',
                'content_type': 'impact_report'
            }
        }
    )

    results = await engine.evaluate_async(executive_context, mode="behaviour")
    print_rule_results(results)

    # Test case 2: Data integrity validation
    print("\nüß™ Test Case 2: Data Integrity Check")
    data_context = ExecutionContext(
        context_type=ContextType.DATA_VALIDATION,
        context_id=f"data_{int(time.time())}",
        data={
            'project_id': 'movember',
            'data': {
                'source': 'unverified_external',
                'confidence': 0.7,
                'id': 'data_123'
            },
            'operation': {
                'type': 'impact_analysis',
                'sensitivity_level': 'high'
            }
        }
    )

    results = await engine.evaluate_async(data_context, mode="validation")
    print_rule_results(results)

    # Test case 3: Mission alignment validation
    print("\nüß™ Test Case 3: Mission Alignment")
    mission_context = ExecutionContext(
        context_type=ContextType.BUSINESS_PROCESS,
        context_id=f"mission_{int(time.time())}",
        data={
            'project_id': 'movember',
            'output': {
                'mission_alignment_score': 0.6,
                'context_type': 'impact_report',
                'content': 'mental_health_awareness_campaign'
            }
        }
    )

    results = await engine.evaluate_async(mission_context, mode="mission_check")
    print_rule_results(results)

    return engine


async def demo_impact_reporting_rules():
    """Demonstrate impact reporting rules."""
    print_separator("IMPACT REPORTING RULES DEMO")

    # Create Movember AI engine
    engine = create_movember_engine()

    print("üìä Testing Impact Reporting Rules")
    print("Rules ensure proper framework alignment, outcome mapping, and data visualization")

    # Test case 1: Framework alignment requirement
    print("\nüß™ Test Case 1: Framework Alignment")
    framework_context = ExecutionContext(
        context_type=ContextType.BUSINESS_PROCESS,
        context_id=f"framework_{int(time.time())}",
        data={
            'project_id': 'movember',
            'report': {
                'type': 'impact',
                'framework': 'custom_framework',  # Not in allowed list
                'outputs': ['awareness_campaign', 'community_engagement'],
                'outcomes': []  # Missing outcomes
            }
        }
    )

    results = await engine.evaluate_async(framework_context, mode="reporting")
    print_rule_results(results)

    # Test case 2: Output-outcome mapping
    print("\nüß™ Test Case 2: Output-Outcome Mapping")
    mapping_context = ExecutionContext(
        context_type=ContextType.BUSINESS_PROCESS,
        context_id=f"mapping_{int(time.time())}",
        data={
            'project_id': 'movember',
            'report': {
                'type': 'impact',
                'framework': 'ToC',
                'outputs': ['training_sessions', 'awareness_materials'],
                'outcomes': ['increased_knowledge', 'behavior_change'],
                'visualizations': [],
                'data_points': 10
            }
        }
    )

    results = await engine.evaluate_async(mapping_context, mode="reporting")
    print_rule_results(results)

    # Test case 3: SDG alignment
    print("\nüß™ Test Case 3: SDG Alignment")
    sdg_context = ExecutionContext(
        context_type=ContextType.BUSINESS_PROCESS,
        context_id=f"sdg_{int(time.time())}",
        data={
            'project_id': 'movember',
            'report': {
                'type': 'impact',
                'framework': 'SDG',
                'global_impact': True,
                'sdg_mapping': [],
                'impact_claims': ['improved_health_outcomes'],
                'attribution_clarity': 'unclear'
            }
        }
    )

    results = await engine.evaluate_async(sdg_context, mode="reporting")
    print_rule_results(results)

    return engine


async def demo_grant_evaluation_rules():
    """Demonstrate grant evaluation rules."""
    print_separator("GRANT EVALUATION RULES DEMO")

    # Create Movember AI engine
    engine = create_movember_engine()

    print("üí∞ Testing Grant Evaluation Rules")
    print("Rules ensure grant completeness, impact linkage, and proper evaluation")

    # Test case 1: Grant application completeness
    print("\nüß™ Test Case 1: Grant Completeness")
    completeness_context = ExecutionContext(
        context_type=ContextType.BUSINESS_PROCESS,
        context_id=f"completeness_{int(time.time())}",
        data={
            'project_id': 'movember',
            'grant': {
                'status': 'submitted',
                'application_fields': {'missing': []},
                'budget': {'total': 50000},
                'timeline': {'start_date': '2024-01-01'},
                'type': 'research',
                'impact_metrics': []
            }
        }
    )

    results = await engine.evaluate_async(completeness_context, mode="grant_evaluation")
    print_rule_results(results)

    # Test case 2: Budget validation
    print("\nüß™ Test Case 2: Budget Validation")
    budget_context = ExecutionContext(
        context_type=ContextType.BUSINESS_PROCESS,
        context_id=f"budget_{int(time.time())}",
        data={
            'project_id': 'movember',
            'grant': {
                'type': 'research',
                'budget': {
                    'total': 200000,
                    'justification': 'Brief justification'
                },
                'category': {'max_amount': 100000}
            }
        }
    )

    results = await engine.evaluate_async(budget_context, mode="grant_evaluation")
    print_rule_results(results)

    # Test case 3: SDG alignment for grants
    print("\nüß™ Test Case 3: Grant SDG Alignment")
    grant_sdg_context = ExecutionContext(
        context_type=ContextType.BUSINESS_PROCESS,
        context_id=f"grant_sdg_{int(time.time())}",
        data={
            'project_id': 'movember',
            'grant': {
                'type': 'global',
                'sdg_mapping': [],
                'evaluation_plan': 'Basic evaluation',
                'monitoring_indicators': ['indicator1'],
                'sustainability_plan': 'Brief plan',
                'long_term_impact': 'Short description'
            }
        }
    )

    results = await engine.evaluate_async(grant_sdg_context, mode="grant_evaluation")
    print_rule_results(results)

    return engine


async def demo_context_validation_rules():
    """Demonstrate context validation rules."""
    print_separator("CONTEXT VALIDATION RULES DEMO")

    # Create Movember AI engine
    engine = create_movember_engine()

    print("üîí Testing Context Validation Rules")
    print("Rules ensure operations are within Movember scope and have proper permissions")

    # Test case 1: Movember context validation
    print("\nüß™ Test Case 1: Movember Context Validation")
    movember_context = ExecutionContext(
        context_type=ContextType.BUSINESS_PROCESS,
        context_id=f"movember_{int(time.time())}",
        data={
            'project_id': 'movember',
            'project': {
                'id': 'movember',
                'context_type': 'movember_impact'
            },
            'operation': {
                'mission_alignment_score': 0.8,
                'context_type': 'impact_analysis'
            }
        }
    )

    results = await engine.evaluate_async(movember_context, mode="context_validation")
    print_rule_results(results)

    # Test case 2: Non-Movember context (should be blocked)
    print("\nüß™ Test Case 2: Non-Movember Context (Should Be Blocked)")
    non_movember_context = ExecutionContext(
        context_type=ContextType.BUSINESS_PROCESS,
        context_id=f"non_movember_{int(time.time())}",
        data={
            'project_id': 'other_project',
            'project': {
                'id': 'other_project',
                'context_type': 'marketing'
            }
        }
    )

    try:
        results = await engine.evaluate_async(non_movember_context, mode="context_validation")
        print_rule_results(results)
    except ValueError as e:
        print(f"‚ùå Correctly blocked: {e}")

    # Test case 3: Stakeholder permissions
    print("\nüß™ Test Case 3: Stakeholder Permissions")
    permissions_context = ExecutionContext(
        context_type=ContextType.BUSINESS_PROCESS,
        context_id=f"permissions_{int(time.time())}",
        data={
            'project_id': 'movember',
            'user': {
                'id': 'user_123',
                'role': 'guest'
            },
            'operation': {
                'type': 'impact_analysis',
                'sensitivity_level': 'high'
            }
        }
    )

    results = await engine.evaluate_async(permissions_context, mode="permissions_check")
    print_rule_results(results)

    return engine


async def demo_weekly_refactoring():
    """Demonstrate weekly refactoring capabilities."""
    print_separator("WEEKLY REFACTORING DEMO")

    print("üîÑ Testing Weekly Refactoring System")
    print("Automated rule maintenance, duplicate detection, and performance optimization")

    # Run weekly refactor
    print("\nüß™ Running Weekly Refactor Process")
    summary = run_weekly_refactor()

    print(f"\nüìä Refactor Summary:")
    print(f"  Total Rules Analyzed: {summary.total_rules}")
    print(f"  Unused Rules Found: {len(summary.unused_rules)}")
    print(f"  Duplicate Groups Found: {len(summary.duplicate_groups)}")
    print(f"  Performance Issues Found: {len(summary.performance_issues)}")

    if summary.unused_rules:
        print(f"\nüö® Unused Rules:")
        for rule in summary.unused_rules[:5]:  # Show first 5
            print(f"  - {rule}")

    if summary.duplicate_groups:
        print(f"\nüîÑ Duplicate Groups:")
        for group in summary.duplicate_groups[:3]:  # Show first 3
            print(f"  - Primary: {group.primary_rule}")
            print(f"    Duplicates: {', '.join(group.duplicate_rules)}")
            print(f"    Similarity: {group.similarity_score:.2f}")

    if summary.performance_issues:
        print(f"\n‚ö° Performance Issues:")
        for issue in summary.performance_issues[:3]:  # Show first 3
            print(f"  - {issue}")

    print(f"\nüìã Recommendations:")
    for i, recommendation in enumerate(summary.recommendations[:5], 1):
        print(f"  {i}. {recommendation}")

    return summary


async def demo_integrated_scenarios():
    """Demonstrate integrated scenarios using the Movember AI Rules System."""
    print_separator("INTEGRATED SCENARIOS DEMO")

    print("üéØ Testing Integrated Scenarios")
    print("Real-world use cases combining multiple rule categories")

    # Scenario 1: Impact Analysis for Executive Report
    print("\nüß™ Scenario 1: Impact Analysis for Executive Report")

    impact_data = {
        'project_id': 'movember',
        'analysis_type': 'impact',
        'framework': 'ToC',
        'outputs': ['awareness_campaign', 'community_engagement', 'training_sessions'],
        'outcomes': ['increased_knowledge', 'behavior_change', 'health_improvements'],
        'stakeholder': 'executive',
        'data_visualization': True,
        'attribution_clarity': 'clear'
    }

    impact_results = run_movember_impact_analysis(impact_data)
    print(f"‚úÖ Impact Analysis Results:")
    print(f"  Framework Compliance: {impact_results['framework_compliance']}")
    print(f"  Outcome Mapping: {impact_results['outcome_mapping']}")
    print(f"  Data Visualization: {impact_results['data_visualization']}")

    # Scenario 2: Grant Application Evaluation
    print("\nüß™ Scenario 2: Grant Application Evaluation")

    grant_data = {
        'project_id': 'movember',
        'grant_type': 'research',
        'budget': 75000,
        'timeline': '18 months',
        'impact_metrics': ['publications', 'policy_influence'],
        'sdg_alignment': ['SDG3', 'SDG4'],
        'evaluation_plan': 'Comprehensive evaluation with baseline and follow-up',
        'partnerships': ['university_partner', 'community_organization']
    }

    grant_results = evaluate_grant_application(grant_data)
    print(f"‚úÖ Grant Evaluation Results:")
    print(f"  Completeness Score: {grant_results['completeness_score']:.1%}")
    print(f"  Impact Metrics Present: {grant_results['impact_metrics_present']}")
    print(f"  Budget Appropriate: {grant_results['budget_appropriate']}")
    print(f"  Timeline Realistic: {grant_results['timeline_realistic']}")

    # Scenario 3: Operation Validation
    print("\nüß™ Scenario 3: Operation Validation")

    operation_data = {
        'project_id': 'movember',
        'operation_type': 'mental_health_analysis',
        'stakeholder_audience': 'funder',
        'data_sources': ['movember_database', 'verified_external'],
        'mission_focus': ['mental_health', 'awareness'],
        'strategy_pillars': ['prevention', 'support']
    }

    validation_results = validate_movember_operation(operation_data)
    print(f"‚úÖ Operation Validation Results:")
    print(f"  Is Valid: {validation_results['is_valid']}")
    print(f"  Mission Alignment: {validation_results['mission_alignment']['is_aligned']}")
    print(f"  Alignment Score: {validation_results['mission_alignment']['alignment_score']:.1%}")

    return {
        'impact_analysis': impact_results,
        'grant_evaluation': grant_results,
        'operation_validation': validation_results
    }


async def demo_system_metrics():
    """Demonstrate system metrics and monitoring."""
    print_separator("SYSTEM METRICS DEMO")

    # Create engine and run some operations
    engine = create_movember_engine()

    # Run a few operations to generate metrics
    test_contexts = [
        ExecutionContext(
            context_type=ContextType.BUSINESS_PROCESS,
            context_id=f"test_{i}",
            data={'project_id': 'movember', 'test_data': f'value_{i}'}
        )
        for i in range(5)
    ]

    for context in test_contexts:
        await engine.evaluate_async(context, mode="test")

    # Get system metrics
    metrics = engine.get_metrics()
    summary = engine.get_system_summary()

    print("üìä System Metrics:")
    if metrics:
        system_metrics = metrics.get('system', {})
        print(f"  Total Rules Executed: {system_metrics.get('total_rules_executed', 0)}")
        print(f"  Total Batch Executions: {system_metrics.get('total_batch_executions', 0)}")
        print(f"  Average Batch Time: {system_metrics.get('average_batch_time', 0):.3f}s")
        print(f"  System Uptime: {system_metrics.get('uptime_seconds', 0):.1f}s")

    print(f"\nüìã System Summary:")
    print(f"  Total Rules: {summary['total_rules']}")
    print(f"  Rule Categories: {summary['rule_categories']}")
    print(f"  Priority Distribution: {summary['priority_distribution']}")
    print(f"  System Status: {summary['system_status']}")

    # Get execution history
    history = engine.get_execution_history(limit=3)
    print(f"\nüìú Recent Execution History:")
    for entry in history:
        print(f"  {entry['timestamp']}: {entry['total_rules']} rules, {entry['successful_rules']} successful")

    return {'metrics': metrics, 'summary': summary, 'history': history}


async def main():
    """Run all demos."""
    print("üöÄ MOVEMBER AI RULES SYSTEM v1.1 DEMO")
    print("=" * 70)
    print("This demo showcases the comprehensive Movember AI Rules System:")
    print("‚Ä¢ AI Behaviour Rules - Professional tone and data integrity")
    print("‚Ä¢ Impact Reporting Rules - Framework alignment and outcome mapping")
    print("‚Ä¢ Grant Evaluation Rules - Completeness and impact linkage")
    print("‚Ä¢ Context Validation Rules - Scope and permission management")
    print("‚Ä¢ Weekly Refactoring - Automated maintenance and optimization")
    print("‚Ä¢ Integrated Scenarios - Real-world use cases")
    print("‚Ä¢ System Metrics - Performance monitoring and analytics")

    # Run all demos
    results = {}

    try:
        results['behaviour'] = await demo_ai_behaviour_rules()
        results['reporting'] = await demo_impact_reporting_rules()
        results['grant'] = await demo_grant_evaluation_rules()
        results['context'] = await demo_context_validation_rules()
        results['refactor'] = await demo_weekly_refactoring()
        results['integrated'] = await demo_integrated_scenarios()
        results['metrics'] = await demo_system_metrics()

        print_separator("DEMO COMPLETE")
        print("‚úÖ All demos completed successfully!")
        print("\nüéâ The Movember AI Rules System v1.1 is working brilliantly!")
        print("\nKey features demonstrated:")
        print("‚Ä¢ 50+ specialized rules across 5 categories")
        print("‚Ä¢ Context-aware validation and permissions")
        print("‚Ä¢ Framework compliance (ToC, CEMP, SDG)")
        print("‚Ä¢ Automated weekly maintenance and refactoring")
        print("‚Ä¢ Comprehensive metrics and monitoring")
        print("‚Ä¢ Real-world scenario integration")
        print("‚Ä¢ Mission alignment and stakeholder focus")

        print(f"\nüìä Demo Summary:")
        print(f"  Total Rule Categories: 5")
        print(f"  Total Rules Tested: 50+")
        print(f"  Scenarios Demonstrated: 7")
        print(f"  Integration Points: 15+")

    except Exception as e:
        print(f"\n‚ùå Demo failed with error: {e}")
        raise

    finally:
        # Cleanup
        for engine in results.values():
            if hasattr(engine, 'shutdown'):
                engine.shutdown()


if __name__ == "__main__":
    asyncio.run(main())
